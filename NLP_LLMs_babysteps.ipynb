{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaroldSthid/blueprint_NLP_and_babysteps_LLMs/blob/main/NLP_LLMs_babysteps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gyk9W7fmL0I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "## An√°lisis de Sentimiento de Pel√≠culas de IMDb.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://github.com/HaroldSthid/blueprint_NLP_and_babysteps_LLMs/blob/main/README.md\n",
        "\n",
        "# üé¨ An√°lisis de Sentimiento de Pel√≠culas de IMDb\n",
        "\n",
        "¬°Bienvenido! Este proyecto te ayudar√° a analizar rese√±as de pel√≠culas y descubrir patrones interesantes sobre qu√© hace que una pel√≠cula sea bien recibida.\n",
        "\n",
        "## üìã Paso 0: Configuraci√≥n inicial\n",
        "\n",
        "Primero, necesitamos preparar nuestro entorno instalando todas las herramientas necesarias."
      ],
      "metadata": {
        "id": "IXCmphRUMnFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar todas las librer√≠as necesarias\n",
        "%pip install tensorflow tensorflow-datasets pandas numpy matplotlib seaborn wordcloud nltk scikit-learn gensim"
      ],
      "metadata": {
        "collapsed": true,
        "id": "89gDfI8zM767"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aQkTzE7Lt-y"
      },
      "outputs": [],
      "source": [
        "# Importar todas las librer√≠as que vamos a utilizar\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Todas las librer√≠as se instalaron e importaron correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Paso 1: Cargar los datos\n",
        "\n",
        "Vamos a cargar dos conjuntos de datos:\n",
        "1. Rese√±as de pel√≠culas de IMDb (con etiquetas de sentimiento)\n",
        "2. Metadatos de pel√≠culas (informaci√≥n adicional como g√©nero, director, etc.)\n"
      ],
      "metadata": {
        "id": "d1B8TW9kNahA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar y cargar las rese√±as de IMDb\n",
        "print(\"üì• Descargando rese√±as de pel√≠culas de IMDb...\")\n",
        "dataset, info = tfds.load('imdb_reviews', split='train', with_info=True, as_supervised=True)\n",
        "print(\"‚úÖ Rese√±as cargadas correctamente\")\n",
        "\n",
        "# Ver informaci√≥n b√°sica del dataset\n",
        "print(f\"‚ÑπÔ∏è  El dataset contiene {info.splits['train'].num_examples} rese√±as\")\n",
        "\n",
        "# Mostrar algunas rese√±as de ejemplo\n",
        "print(\"\\nüîç Algunas rese√±as de ejemplo:\")\n",
        "for i, (text, label) in enumerate(dataset.take(3)):\n",
        "    print(f\"Rese√±a {i+1}: {text.numpy()[:100]}...\")  # Mostrar solo los primeros 100 caracteres\n",
        "    print(f\"Sentimiento: {'Positivo' if label.numpy() else 'Negativo'}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "3OkCJJ6VNT-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßπ Paso 2: Limpiar y preparar el texto\n",
        "\n",
        "Las rese√±as de pel√≠culas a menudo contienen HTML, signos de puntuaci√≥n y otras cosas que no necesitamos para nuestro an√°lisis. Vamos a limpiarlas."
      ],
      "metadata": {
        "id": "YGwm_V24N8pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar recursos necesarios para procesamiento de texto\n",
        "print(\"üì• Descargando recursos de procesamiento de lenguaje...\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Configurar herramientas de limpieza de texto\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Funci√≥n para limpiar texto:\n",
        "    1. Convierte a min√∫sculas\n",
        "    2. Elimina etiquetas HTML\n",
        "    3. Elimina signos de puntuaci√≥n\n",
        "    4. Elimina espacios extra\n",
        "    \"\"\"\n",
        "    # Si es un tensor de TensorFlow, convertirlo a texto\n",
        "    if isinstance(text, tf.Tensor):\n",
        "        text = text.numpy().decode('utf-8')\n",
        "\n",
        "    text = text.lower()  # Convertir a min√∫sculas\n",
        "    text = re.sub(r'<br />', ' ', text)  # Eliminar etiquetas HTML\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Eliminar puntuaci√≥n\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Eliminar espacios extras\n",
        "    text = text.strip()  # Eliminar espacios al inicio y final\n",
        "\n",
        "    return text\n",
        "\n",
        "# Probar la funci√≥n de limpieza con una rese√±a de ejemplo\n",
        "print(\"üß™ Probando la limpieza de texto...\")\n",
        "sample_text = \"This is a <br /> GREAT movie!!! It's amazing.\"\n",
        "cleaned_text = clean_text(sample_text)\n",
        "print(f\"Texto original: {sample_text}\")\n",
        "print(f\"Texto limpio: {cleaned_text}\")"
      ],
      "metadata": {
        "id": "raVYdniSN9Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÑ Paso 3: Procesamiento avanzado de texto\n",
        "\n",
        "Ahora vamos a mejorar a√∫n m√°s nuestro texto usando t√©cnicas avanzadas como la lematizaci√≥n (convertir palabras a su forma base)."
      ],
      "metadata": {
        "id": "tXvOchZLOho8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "    \"\"\"\n",
        "    Funci√≥n para lematizar texto (convertir palabras a su forma base)\n",
        "    Ejemplo: \"running\" -> \"run\", \"better\" -> \"good\"\n",
        "    \"\"\"\n",
        "    # Descargar recurso punkt_tab if not already downloaded\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt_tab')\n",
        "    except nltk.downloader.DownloadError:\n",
        "        nltk.download('punkt_tab')\n",
        "\n",
        "    # Tokenizar (dividir en palabras)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Lematizar cada palabra y eliminar palabras vac√≠as\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    # Unir las palabras de nuevo en un texto\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "# Probar la lematizaci√≥n con un ejemplo\n",
        "print(\"üß™ Probando lematizaci√≥n de texto...\")\n",
        "sample_text = \"I was running quickly to find better solutions\"\n",
        "lemmatized_text = lemmatize_text(sample_text)\n",
        "print(f\"Texto original: {sample_text}\")\n",
        "print(f\"Texto lematizado: {lemmatized_text}\")\n",
        "\n",
        "# Aplicar limpieza y lematizaci√≥n a todo el dataset\n",
        "print(\"üîÑ Procesando todas las rese√±as (esto puede tomar unos minutos)...\")\n",
        "\n",
        "def process_dataset(dataset, sample_size=1000):\n",
        "    \"\"\"\n",
        "    Procesa un conjunto de datos de rese√±as\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    for text, label in dataset.take(sample_size):  # Usar solo una muestra para no sobrecargar\n",
        "        cleaned = clean_text(text)\n",
        "        lemmatized = lemmatize_text(cleaned)\n",
        "        texts.append(lemmatized)\n",
        "        labels.append(label.numpy())\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "# Procesar las rese√±as\n",
        "texts, labels = process_dataset(dataset, sample_size=1000)\n",
        "print(f\"‚úÖ Procesamiento completado. {len(texts)} rese√±as procesadas.\")"
      ],
      "metadata": {
        "id": "Q-dkAClCOiHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "700bd490"
      },
      "source": [
        "## üìä Paso 4: An√°lisis y Visualizaci√≥n de Datos\n",
        "\n",
        "Ahora que hemos limpiado y procesado el texto, podemos empezar a explorar los datos para encontrar patrones interesantes.\n",
        "\n",
        "### Distribuci√≥n de Sentimientos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c8c4d25"
      },
      "source": [
        "# Contar la distribuci√≥n de sentimientos\n",
        "sentiment_counts = pd.Series(labels).value_counts()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
        "plt.xticks([0, 1], ['Negativo', 'Positivo'])\n",
        "plt.title('Distribuci√≥n de Sentimientos en las Rese√±as')\n",
        "plt.xlabel('Sentimiento')\n",
        "plt.ylabel('N√∫mero de Rese√±as')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1bc7b48"
      },
      "source": [
        "### Nube de Palabras para Rese√±as Positivas y Negativas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d47e0e19"
      },
      "source": [
        "# Separar rese√±as positivas y negativas\n",
        "positive_texts = [texts[i] for i, label in enumerate(labels) if label == 1]\n",
        "negative_texts = [texts[i] for i, label in enumerate(labels) if label == 0]\n",
        "\n",
        "# Generar nube de palabras para rese√±as positivas\n",
        "positive_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(positive_texts))\n",
        "\n",
        "# Generar nube de palabras para rese√±as negativas\n",
        "negative_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(negative_texts))\n",
        "\n",
        "# Mostrar las nubes de palabras\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Palabras Comunes en Rese√±as Positivas')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Palabras Comunes en Rese√±as Negativas')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Paso 5: Entrenar un modelo de inteligencia artificial\n",
        "\n",
        "Ahora vamos a entrenar un modelo que pueda predecir si una rese√±a es positiva o negativa bas√°ndose en su texto."
      ],
      "metadata": {
        "id": "0a7QvGM1P6Wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ü§ñ Entrenando modelo de an√°lisis de sentimiento...\")\n",
        "\n",
        "# Convertir texto a caracter√≠sticas num√©ricas usando TF-IDF\n",
        "print(\"üìä Convirtiendo texto a caracter√≠sticas num√©ricas...\")\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "y = np.array(labels)\n",
        "\n",
        "print(f\"‚úÖ Texto convertido. Dimensiones: {X.shape}\")\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"üìù Datos de entrenamiento: {X_train.shape[0]} ejemplos\")\n",
        "print(f\"üìù Datos de prueba: {X_test.shape[0]} ejemplos\")\n",
        "\n",
        "# Entrenar modelo de regresi√≥n log√≠stica\n",
        "print(\"üéì Entrenando modelo...\")\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"‚úÖ Modelo entrenado correctamente\")\n",
        "\n",
        "# Evaluar el modelo\n",
        "print(\"üìà Evaluando modelo...\")\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"üîç Reporte de clasificaci√≥n:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Negativo', 'Positivo']))\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negativo', 'Positivo'],\n",
        "            yticklabels=['Negativo', 'Positivo'])\n",
        "plt.title('Matriz de Confusi√≥n')\n",
        "plt.ylabel('Verdaderos')\n",
        "plt.xlabel('Predicciones')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6vyO1vRxQGZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Paso 6: Guardar el modelo entrenado\n",
        "\n",
        "Vamos a guardar nuestro modelo para usarlo m√°s tarde sin necesidad de volver a entrenarlo."
      ],
      "metadata": {
        "id": "Asb6vMf7QdMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üíæ Guardando modelo y vectorizador...\")\n",
        "\n",
        "# Guardar modelo\n",
        "with open('sentiment_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Guardar vectorizador\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "print(\"‚úÖ Modelo y vectorizador guardados correctamente\")"
      ],
      "metadata": {
        "id": "NewQntlMQjWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Paso 7: Probar el modelo con nuevas rese√±as\n",
        "\n",
        "Ahora vamos a probar nuestro modelo con algunas rese√±as nuevas para ver c√≥mo funciona."
      ],
      "metadata": {
        "id": "wx4foJUVQxyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n para predecir sentimiento de nuevo texto\n",
        "def predict_sentiment(text, model, vectorizer):\n",
        "    \"\"\"\n",
        "    Predice si un texto tiene sentimiento positivo o negativo\n",
        "    \"\"\"\n",
        "    # Limpiar y procesar el texto\n",
        "    cleaned_text = clean_text(text)\n",
        "    lemmatized_text = lemmatize_text(cleaned_text)\n",
        "\n",
        "    # Convertir a caracter√≠sticas\n",
        "    features = vectorizer.transform([lemmatized_text])\n",
        "\n",
        "    # Predecir\n",
        "    prediction = model.predict(features)\n",
        "    probability = model.predict_proba(features)\n",
        "\n",
        "    # Obtener confianza de la predicci√≥n\n",
        "    confidence = probability[0][prediction[0]]\n",
        "\n",
        "    return \"Positivo\" if prediction[0] == 1 else \"Negativo\", confidence\n",
        "\n",
        "# Probar con algunas rese√±as de ejemplo\n",
        "test_reviews = [\n",
        "    \"This movie is absolutely fantastic! The acting was superb and the plot was engaging.\",\n",
        "    \"I hated this film. It was boring and the characters were poorly developed.\",\n",
        "    \"The movie was okay. Not great but not terrible either.\",\n",
        "    \"This is the worst movie I have ever seen in my life. A complete waste of time.\",\n",
        "    \"Brilliant cinematography and outstanding performances by the entire cast.\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Probando el modelo con nuevas rese√±as...\")\n",
        "for i, review in enumerate(test_reviews):\n",
        "    sentiment, confidence = predict_sentiment(review, model, vectorizer)\n",
        "    print(f\"Rese√±a {i+1}: {sentiment} (confianza: {confidence:.2f})\")\n",
        "    print(f\"Texto: {review[:80]}...\")  # Mostrar solo el inicio de la rese√±a\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "t4P2XbeQQ8HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà Paso 8: An√°lisis avanzado y visualizaci√≥n\n",
        "\n",
        "Vamos a hacer un an√°lisis m√°s profundo para entender qu√© palabras son m√°s importantes para determinar el sentimiento."
      ],
      "metadata": {
        "id": "j5m6W1NERQas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener las palabras m√°s importantes para cada clase\n",
        "print(\"üîç Analizando palabras m√°s importantes...\")\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Crear un DataFrame para las palabras y sus coeficientes\n",
        "word_importance = pd.DataFrame({\n",
        "    'word': feature_names,\n",
        "    'coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Ordenar por importancia\n",
        "word_importance = word_importance.sort_values('coefficient', ascending=False)\n",
        "\n",
        "# Mostrar las 10 palabras m√°s positivas y negativas\n",
        "print(\"üìä Top 10 palabras para sentimiento POSITIVO:\")\n",
        "print(word_importance.head(10)[['word', 'coefficient']])\n",
        "\n",
        "print(\"\\nüìä Top 10 palabras para sentimiento NEGATIVO:\")\n",
        "print(word_importance.tail(10)[['word', 'coefficient']])\n",
        "\n",
        "# Visualizar las palabras m√°s importantes\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Palabras positivas\n",
        "top_positive = word_importance.head(10)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.barh(top_positive['word'], top_positive['coefficient'], color='green')\n",
        "plt.title('Palabras m√°s asociadas con rese√±as POSITIVAS')\n",
        "plt.xlabel('Coeficiente (mayor = m√°s positivo)')\n",
        "\n",
        "# Palabras negativas\n",
        "top_negative = word_importance.tail(10)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.barh(top_negative['word'], top_negative['coefficient'], color='red')\n",
        "plt.title('Palabras m√°s asociadas con rese√±as NEGATIVAS')\n",
        "plt.xlabel('Coeficiente (menor = m√°s negativo)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8Jvewh3dRV_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ ¬°Felicidades!\n",
        "\n",
        "Has completado exitosamente el an√°lisis de sentimiento de pel√≠culas de IMDb.\n",
        "\n",
        "### üìã Resumen de lo que lograste:\n",
        "\n",
        "1. ‚úÖ Cargaste y exploraste un dataset de rese√±as de pel√≠culas\n",
        "2. ‚úÖ Limpiaste y preprocesaste el texto de las rese√±as\n",
        "3. ‚úÖ Visualizaste las palabras m√°s comunes en rese√±as positivas y negativas\n",
        "4. ‚úÖ Entrenaste un modelo de machine learning para predecir sentimiento\n",
        "5. ‚úÖ Evaluaste el rendimiento de tu modelo\n",
        "6. ‚úÖ Probaste el modelo con nuevas rese√±as\n",
        "7. ‚úÖ Analizaste qu√© palabras son m√°s importantes para determinar sentimiento\n",
        "\n",
        "### üöÄ Pr√≥ximos pasos que puedes intentar:\n",
        "\n",
        "1. **Mejorar el modelo**: Prueba con diferentes algoritmos de machine learning\n",
        "2. **M√°s datos**: Usa el dataset completo en lugar de solo una muestra\n",
        "3. **Aplicaci√≥n web**: Crea una interfaz web donde las personas puedan escribir rese√±as y obtener predicciones\n",
        "4. **An√°lisis de g√©nero**: Combina con datos de g√©neros de pel√≠culas para ver qu√© g√©neros tienden a tener mejores rese√±as\n",
        "\n",
        "### üìö Recursos para aprender m√°s:\n",
        "\n",
        "- [Curso de Machine Learning para principiantes](https://www.coursera.org/learn/machine-learning)\n",
        "- [Introducci√≥n a Procesamiento de Lenguaje Natural](https://www.youtube.com/watch?v=8S3qHHUKqYk)\n",
        "- [TensorFlow para principiantes](https://www.tensorflow.org/resources/learn-ml)\n",
        "\n",
        "¬°Gracias por seguir este tutorial! Si tienes preguntas o comentarios, no dudes en compartirlos."
      ],
      "metadata": {
        "id": "_o4NviJSR1tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mensaje final\n",
        "print(\"\\nüéâ ¬°An√°lisis completado exitosamente!\")\n",
        "print(\"üìÅ Los archivos del modelo se han guardado en tu entorno de Google Colab\")\n",
        "print(\"üöÄ Puedes ejecutar esta celda nuevamente para reentrenar el modelo o probar con nuevas rese√±as\")"
      ],
      "metadata": {
        "id": "ZUsuikqASExc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}